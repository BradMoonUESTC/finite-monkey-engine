import os
import re
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

from .utils.scan_utils import ScanUtils
from prompt_factory.vul_prompt_common import VulPromptCommon
from prompt_factory.periphery_prompt import PeripheryPrompt
from prompt_factory.core_prompt import CorePrompt
from prompt_factory.assumption_validation_prompt import AssumptionValidationPrompt
from prompt_factory.vul_reasoning_json_prompt import VulReasoningJsonPrompt
from prompt_factory.prompt_assembler import PromptAssembler
from openai_api.openai import analyze_code_assumptions
from logging_config import get_logger
import json
from dao.entity import Project_Finding
from dao.finding_mgr import ProjectFindingMgr
from codex_service import CodexClient
from codex_runner import CodexCliError


class VulnerabilityScanner:
    """æ¼æ´æ‰«æå™¨ï¼Œè´Ÿè´£æ™ºèƒ½åˆçº¦ä»£ç çš„æ¼æ´æ‰«æ"""
    
    def __init__(self, project_audit, codex_client: Optional[CodexClient] = None):
        self.project_audit = project_audit
        self.logger = get_logger(f"VulnerabilityScanner[{project_audit.project_id}]")
        self.codex_client = codex_client or CodexClient()
        
        # ğŸ¯ è¯»å–é¡¹ç›®è®¾è®¡æ–‡æ¡£ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.design_doc_content = self._load_design_document()
        
        # ğŸ¯ è¯»å–å›ºå®šä¸å˜é‡åˆ—è¡¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.fixed_invariants = self._load_fixed_invariants()
    
    def _load_design_document(self) -> str:
        """åŠ è½½é¡¹ç›®è®¾è®¡æ–‡æ¡£å†…å®¹"""
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨è®¾è®¡æ–‡æ¡£ä¸Šä¸‹æ–‡
        enable_design_doc = os.getenv("ENABLE_DESIGN_DOC_CONTEXT", "False").lower() == "true"
        
        if not enable_design_doc:
            return ""
        
        # è·å–è®¾è®¡æ–‡æ¡£è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
        doc_path = os.getenv("PROJECT_DESIGN_DOC_PATH", "project_design.md")
        
        # å°è¯•è¯»å–æ–‡æ¡£
        try:
            # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆå‡è®¾scanner.pyåœ¨src/reasoning/ä¸‹ï¼‰
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.abspath(os.path.join(current_dir, "..", ".."))
            full_path = os.path.join(project_root, doc_path)
            
            if os.path.exists(full_path):
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    if content:
                        self.logger.info(f"âœ… æˆåŠŸåŠ è½½é¡¹ç›®è®¾è®¡æ–‡æ¡£: {doc_path} ({len(content)} å­—ç¬¦)")
                        return content
                    else:
                        self.logger.warning(f"âš ï¸ è®¾è®¡æ–‡æ¡£ä¸ºç©º: {doc_path}")
                        return ""
            else:
                self.logger.warning(f"âš ï¸ è®¾è®¡æ–‡æ¡£ä¸å­˜åœ¨: {full_path}")
                return ""
                
        except Exception as e:
            self.logger.error(f"âŒ è¯»å–è®¾è®¡æ–‡æ¡£å¤±è´¥: {e}")
            return ""
    
    def _load_fixed_invariants(self) -> list:
        """åŠ è½½å›ºå®šä¸å˜é‡åˆ—è¡¨"""
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨å›ºå®šä¸å˜é‡
        enable_fixed_invariants = os.getenv("ENABLE_FIXED_INVARIANTS", "False").lower() == "true"
        
        if not enable_fixed_invariants:
            return []
        
        # è·å–å›ºå®šä¸å˜é‡æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
        invariants_path = os.getenv("FIXED_INVARIANTS_PATH", "fixed_invariants.md")
        
        # å°è¯•è¯»å–æ–‡ä»¶
        try:
            # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆå‡è®¾scanner.pyåœ¨src/reasoning/ä¸‹ï¼‰
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.abspath(os.path.join(current_dir, "..", ".."))
            full_path = os.path.join(project_root, invariants_path)
            
            if os.path.exists(full_path):
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    if content:
                        # ä½¿ç”¨<|INVARIANT_SPLIT|>åˆ†å‰²ç¬¦è§£æå›ºå®šä¸å˜é‡
                        invariants_raw = content.split("<|INVARIANT_SPLIT|>")
                        invariants_list = []
                        for inv in invariants_raw:
                            cleaned_inv = inv.strip()
                            # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²å’ŒåªåŒ…å«æ ‡é¢˜/æ³¨é‡Šçš„éƒ¨åˆ†
                            if cleaned_inv and not cleaned_inv.startswith('#') and len(cleaned_inv) > 50:
                                invariants_list.append(cleaned_inv)
                        
                        if invariants_list:
                            self.logger.info(f"âœ… æˆåŠŸåŠ è½½å›ºå®šä¸å˜é‡: {invariants_path} ({len(invariants_list)} ä¸ªä¸å˜é‡)")
                            return invariants_list
                        else:
                            self.logger.warning(f"âš ï¸ å›ºå®šä¸å˜é‡æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„ä¸å˜é‡: {invariants_path}")
                            return []
                    else:
                        self.logger.warning(f"âš ï¸ å›ºå®šä¸å˜é‡æ–‡ä»¶ä¸ºç©º: {invariants_path}")
                        return []
            else:
                self.logger.warning(f"âš ï¸ å›ºå®šä¸å˜é‡æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
                return []
                
        except Exception as e:
            self.logger.error(f"âŒ è¯»å–å›ºå®šä¸å˜é‡å¤±è´¥: {e}")
            return []

    def do_scan(self, task_manager, is_gpt4=False, filter_func=None):
        """æ‰§è¡Œæ¼æ´æ‰«æ"""
        # è·å–ä»»åŠ¡åˆ—è¡¨
        tasks = task_manager.get_task_list()
        if len(tasks) == 0:
            return []

        print("ğŸ”„ æ ‡å‡†æ¨¡å¼è¿è¡Œä¸­")
        return self._scan_standard_mode(tasks, task_manager, filter_func, is_gpt4)

    def _scan_standard_mode(self, tasks, task_manager, filter_func, is_gpt4):
        """æ ‡å‡†æ¨¡å¼æ‰«æ
        
        æ‰§è¡Œç­–ç•¥ï¼š
        1. æŒ‰ group åˆ†ç»„ä»»åŠ¡
        2. åŒä¸€ä¸ª group å†…çš„ä»»åŠ¡ä¸²è¡Œæ‰§è¡Œï¼ˆä¿è¯åŒç»„æ€»ç»“çš„é¡ºåºæ€§ï¼‰
        3. ä¸åŒ group ä¹‹é—´å¹¶è¡Œæ‰§è¡Œï¼ˆæå‡æ•´ä½“æ•ˆç‡ï¼‰
        """
        max_threads = int(os.getenv("MAX_THREADS_OF_SCAN", 5))
        
        # æŒ‰ group åˆ†ç»„ä»»åŠ¡
        group_dict = {}
        for task in tasks:
            group_uuid = getattr(task, 'group', '') or 'no_group'
            if group_uuid not in group_dict:
                group_dict[group_uuid] = []
            group_dict[group_uuid].append(task)
        
        # ä¸ºæ¯ä¸ª group å®šä¹‰å¤„ç†å‡½æ•°ï¼ˆä¸²è¡Œå¤„ç† group å†…çš„ä»»åŠ¡ï¼‰
        def process_group(group_tasks):
            for task in group_tasks:
                self._process_single_task_standard(task, task_manager, filter_func, is_gpt4)
        
        # å¹¶è¡Œå¤„ç†ä¸åŒçš„ group
        group_list = list(group_dict.values())
        ScanUtils.execute_parallel_scan(group_list, process_group, max_threads)
        return tasks

    def _execute_vulnerability_scan(self, task, task_manager, is_gpt4: bool) -> str:
        """æ‰§è¡Œæ¼æ´æ‰«æï¼ˆä½¿ç”¨ä»»åŠ¡ä¸­å·²ç¡®å®šçš„ruleï¼‰
        
        æ³¨æ„ï¼šç°åœ¨ç»Ÿä¸€ä½¿ç”¨vulnerability_detectioné…ç½®(claude4sonnet)ï¼Œis_gpt4å‚æ•°å·²ä¸å†ä½¿ç”¨ä½†ä¿ç•™ä»¥å…¼å®¹
        """
        try:
            # è·å–ä»»åŠ¡çš„business_flow_codeä½œä¸ºä»£ç éƒ¨åˆ†
            business_flow_code = getattr(task, 'business_flow_code', task.content)
            
            # ä»ä»»åŠ¡ä¸­è·å–å·²ç»ç¡®å®šçš„ruleï¼ˆPlanningé˜¶æ®µå·²ç»åˆ†é…å¥½çš„checklistï¼‰
            task_rule = getattr(task, 'rule', '')
            rule_key = getattr(task, 'rule_key', '')
            
            # è§£ærule
            rule_list = []
            if task_rule:
                # ğŸ¯ assumption_violationç±»å‹çš„ä»»åŠ¡ï¼Œruleæ˜¯åˆ—è¡¨æ ¼å¼ï¼ˆåˆ†ç»„çš„assumption/invariantï¼‰
                if rule_key == "assumption_violation":
                    rule_list = task_rule  # ç›´æ¥ä½¿ç”¨åˆ—è¡¨
                else:
                    # å…¶ä»–ç±»å‹ä»»åŠ¡ï¼Œå°è¯•è§£æJSONæ ¼å¼
                    try:
                        rule_list = json.loads(task_rule)
                    except json.JSONDecodeError as e:
                        self.logger.warning(f"ä»»åŠ¡ {task.name} çš„ruleè§£æå¤±è´¥: {e}")
                        rule_list = []
            
            # ğŸ¯ å°†å›ºå®šä¸å˜é‡æ·»åŠ åˆ°æ£€æŸ¥åˆ—è¡¨ä¸­ï¼ˆä»…å¯¹assumption_violationç±»å‹çš„ä»»åŠ¡ï¼‰
            if rule_key == "assumption_violation" and self.fixed_invariants:
                if isinstance(rule_list, list):
                    # å°†å›ºå®šä¸å˜é‡æ·»åŠ åˆ°ç°æœ‰åˆ—è¡¨ä¸­
                    original_count = len(rule_list)
                    rule_list = rule_list + self.fixed_invariants
                    self.logger.debug(f"ä»»åŠ¡ {task.name} æ·»åŠ äº† {len(self.fixed_invariants)} ä¸ªå›ºå®šä¸å˜é‡ (åŸæœ‰: {original_count}, æ€»è®¡: {len(rule_list)})")
                elif isinstance(rule_list, str):
                    # å¦‚æœrule_listæ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨åæ·»åŠ 
                    rule_list = [rule_list] + self.fixed_invariants
                    self.logger.debug(f"ä»»åŠ¡ {task.name} æ·»åŠ äº† {len(self.fixed_invariants)} ä¸ªå›ºå®šä¸å˜é‡")
                else:
                    # å¦‚æœrule_listä¸ºç©ºæˆ–å…¶ä»–ç±»å‹ï¼Œç›´æ¥ä½¿ç”¨å›ºå®šä¸å˜é‡
                    rule_list = self.fixed_invariants
                    self.logger.debug(f"ä»»åŠ¡ {task.name} ä½¿ç”¨ {len(self.fixed_invariants)} ä¸ªå›ºå®šä¸å˜é‡")
            
            # ğŸ¯ æ–°å¢ï¼šåŸºäºgroupæŸ¥è¯¢åŒç»„å·²æœ‰ç»“æœå¹¶ç”Ÿæˆæ€»ç»“ï¼ˆæ ¹æ®ç¯å¢ƒå˜é‡å¼€å…³æ§åˆ¶ï¼‰
            # ä½ æœ€æ–°å£å¾„ï¼šä¸éœ€è¦åœ¨æ„å†å²æ•°æ®ï¼Œé»˜è®¤å…³é—­åŒç»„æ€»ç»“ï¼ˆå¦‚éœ€å¼€å¯å¯è®¾ SUMMARY_IN_REASONING=Trueï¼‰
            summary_in_reasoning = os.getenv("SUMMARY_IN_REASONING", "False").lower() == "true"
            group_summary = ""
            if summary_in_reasoning:
                group_summary = self._get_group_results_summary(task, task_manager)
            
            # æ‰‹åŠ¨ç»„è£…promptï¼ˆä½¿ç”¨ä»»åŠ¡çš„å…·ä½“ruleè€Œä¸æ˜¯ç´¢å¼•ï¼‰
            assembled_prompt = self._assemble_prompt_with_specific_rule(
                business_flow_code, 
                rule_list, 
                rule_key
            )
            
            # ğŸ¯ å¦‚æœå¯ç”¨äº†é¡¹ç›®è®¾è®¡æ–‡æ¡£ï¼Œå°†å…¶æ·»åŠ åˆ°promptæœ€å‰é¢
            if self.design_doc_content:
                design_doc_prefix = f"""# PROJECT DESIGN CONTEXT

The following is the project's design document, which provides important context about the system's architecture, business logic, and security model. Use this information to better understand the developer's intentions and identify potential vulnerabilities.

{self.design_doc_content}

{"=" * 80}

"""
                assembled_prompt = design_doc_prefix + assembled_prompt
            
            # ğŸ¯ å¦‚æœå¯ç”¨äº†åŒç»„æ€»ç»“ä¸”æœ‰æ€»ç»“å†…å®¹ï¼Œå°†å…¶æ·»åŠ åˆ°promptå‰é¢ï¼ˆåœ¨è®¾è®¡æ–‡æ¡£ä¹‹åï¼‰
            # é»˜è®¤å…³é—­ï¼šSUMMARY_IN_REASONING=False
            if summary_in_reasoning and group_summary:
                from prompt_factory.group_summary_prompt import GroupSummaryPrompt
                enhanced_prefix = GroupSummaryPrompt.get_enhanced_reasoning_prompt_prefix()
                assembled_prompt = enhanced_prefix + group_summary + "\n\n" + "=" * 80 + "\n\n" + assembled_prompt
            
            # ğŸ¯ reasoningé˜¶æ®µï¼šæ”¹ä¸º Codex æ‰§è¡Œï¼ˆagentic workflow + åªè¯»æ£€ç´¢ï¼‰
            project_root = getattr(self.project_audit, "project_path", "") or ""
            if not project_root:
                raise RuntimeError("project_audit.project_path is empty; cannot set codex --cd workspace")

            codex_res = self.codex_client.exec(workspace_root=project_root, prompt=assembled_prompt)
            if codex_res.returncode != 0:
                raise CodexCliError(f"codex reasoning failed: {codex_res.stderr.strip()}")

            result = self._extract_json_object_or_raise(codex_res.stdout)
            
            # ä¿å­˜ç»“æœ
            if hasattr(task, 'id') and task.id:
                task_manager.update_result(task.id, result)
            else:
                self.logger.warning(f"ä»»åŠ¡ {task.name} æ²¡æœ‰IDï¼Œæ— æ³•ä¿å­˜ç»“æœ")

            # ğŸ¯ æ–°å¢ï¼šå°†å¤šæ¼æ´ JSON æ‹†åˆ†å†™å…¥ project_findingï¼ˆå¹‚ç­‰ï¼šæŒ‰ task_id åˆ é™¤æ—§ findings å†é‡å»ºï¼‰
            self._split_and_persist_findings(task, task_manager, result)
            
            print(f"âœ… ä»»åŠ¡ {task.name} æ‰«æå®Œæˆï¼Œä½¿ç”¨rule: {rule_key} ({len(rule_list)}ä¸ªæ£€æŸ¥é¡¹)")
            return result
        except Exception as e:
            print(f"âŒ æ¼æ´æ‰«ææ‰§è¡Œå¤±è´¥: {e}")
            return ""

    @staticmethod
    def _extract_json_object_or_raise(text: str) -> str:
        """
        Codex stdout å¯èƒ½åŒ…å« agentic workflow çš„â€œExplored/exec ...â€ç­‰é JSON æ–‡æœ¬ã€‚
        è¿™é‡Œåšç¨³å¥æå–ï¼šä¼˜å…ˆå– ```json ...```ï¼Œå¦åˆ™å°è¯•ä»æ–‡æœ¬ä¸­è§£æç¬¬ä¸€ä¸ª JSON å¯¹è±¡ã€‚
        è¿”å›ï¼šå¯è¢« json.loads è§£æä¸”ä¸º dict çš„ JSON å­—ç¬¦ä¸²ã€‚
        """
        s = (text or "").strip()
        if not s:
            raise ValueError("empty codex output")

        # fenced json
        m = re.search(r"```json\\s*([\\s\\S]*?)\\s*```", s, re.IGNORECASE)
        if m:
            candidate = m.group(1).strip()
            obj = json.loads(candidate)
            if not isinstance(obj, dict):
                raise ValueError("codex fenced json is not an object")
            return candidate

        # raw decode: find first JSON object
        dec = json.JSONDecoder()
        for i, ch in enumerate(s):
            if ch != "{":
                continue
            try:
                obj, end = dec.raw_decode(s[i:])
                if isinstance(obj, dict):
                    return s[i : i + end]
            except Exception:
                continue

        # fallback: first '{' last '}' slice
        l = s.find("{")
        r = s.rfind("}")
        if l != -1 and r != -1 and r > l:
            candidate = s[l : r + 1]
            obj = json.loads(candidate)
            if isinstance(obj, dict):
                return candidate

        raise ValueError("no json object found in codex output")

    def _process_single_task_standard(self, task, task_manager, filter_func, is_gpt4):
        """æ ‡å‡†æ¨¡å¼å¤„ç†å•ä¸ªä»»åŠ¡"""
        # æ–°ç‰ˆæ–­ç‚¹ç»­è·‘é€»è¾‘ï¼š
        # - result éç©º & short_result == split_done: è·³è¿‡æ‰«æä¸æ‹†åˆ†
        # - result éç©º & short_result != split_done: è·³è¿‡æ‰«æï¼Œä½†éœ€è¦è¡¥æ‹†åˆ†å†™å…¥ finding
        # - result ä¸ºç©º: éœ€è¦æ‰«æï¼ˆscan æ—¶ä¼šè‡ªåŠ¨æ‹†åˆ†ï¼‰
        short_result = getattr(task, 'short_result', '') or ''
        has_result = bool(getattr(task, 'result', '') or '')

        if has_result and short_result == "split_done":
            self.logger.info(f"ä»»åŠ¡ {task.name} å·²æ‰«æä¸”å·²æ‹†åˆ†(split_done)ï¼Œè·³è¿‡")
            return

        if has_result and short_result != "split_done":
            self.logger.info(f"ä»»åŠ¡ {task.name} å·²æ‰«æä½†æœªæ‹†åˆ†ï¼Œæ‰§è¡Œè¡¥æ‹†åˆ†å†™å…¥ finding")
            self._split_and_persist_findings(task, task_manager, task.result)
            return
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ‰«ææ­¤ä»»åŠ¡
        if not ScanUtils.should_scan_task(task, filter_func):
            self.logger.info(f"ä»»åŠ¡ {task.name} ä¸æ»¡è¶³æ‰«ææ¡ä»¶ï¼Œè·³è¿‡")
            return
        
        # æ‰§è¡Œæ¼æ´æ‰«æ
        self._execute_vulnerability_scan(task, task_manager, is_gpt4)

    def _split_and_persist_findings(self, task, task_manager, result_json_text: str):
        """å°† task.result(å¤šæ¼æ´ JSON) æ‹†åˆ†å†™å…¥ project_findingï¼Œå¹¶å°† task.short_result æ ‡è®°ä¸º split_doneï¼ˆå¹‚ç­‰æ–¹æ¡ˆ Aï¼‰ã€‚"""
        try:
            if not getattr(task, 'id', None):
                return

            # å¦‚æœå·²ç»æ ‡è®° split_doneï¼Œåˆ™è·³è¿‡
            if (getattr(task, 'short_result', '') or '') == "split_done":
                return

            # è§£æ JSON
            data = json.loads(result_json_text) if result_json_text else {}
            vulns = data.get("vulnerabilities", []) if isinstance(data, dict) else []

            # æ— æ¼æ´ä¹Ÿè§†ä¸ºæ‹†åˆ†å®Œæˆï¼ˆé¿å…åå¤é‡è¯•ï¼‰
            engine = getattr(task_manager, 'engine', None) or getattr(getattr(task_manager, 'Session', None), 'kw', {}).get('bind', None)
            if engine is None:
                self.logger.warning("æ— æ³•è·å– DB engineï¼Œè·³è¿‡ findings å†™å…¥")
                return
            finding_mgr = ProjectFindingMgr(task_manager.project_id, engine)

            # å¹‚ç­‰ï¼šå…ˆåˆ åå»º
            finding_mgr.delete_findings_by_task_id(task.id)

            findings = []
            for vuln in (vulns or []):
                # å…¼å®¹ä¸¤ç§å½¢å¼ï¼š
                # 1) vuln æ˜¯å­—ç¬¦ä¸² -> è½¬æˆ {"description": "..."}
                # 2) vuln æ˜¯å¯¹è±¡ -> è‹¥æ—  description åˆ™å…œåº•å¡å…¥å­—ç¬¦ä¸²åŒ–å†…å®¹
                if isinstance(vuln, str):
                    vuln_obj = {"description": vuln}
                elif isinstance(vuln, dict):
                    if "description" not in vuln:
                        vuln_obj = {"description": json.dumps(vuln, ensure_ascii=False)}
                    else:
                        vuln_obj = vuln
                else:
                    vuln_obj = {"description": str(vuln)}

                single_json = {
                    "schema_version": data.get("schema_version", "1.0") if isinstance(data, dict) else "1.0",
                    "vulnerabilities": [vuln_obj],
                }
                findings.append(
                    Project_Finding(
                        project_id=task_manager.project_id,
                        task_id=task.id,
                        task_uuid=getattr(task, 'uuid', ''),
                        rule_key=getattr(task, 'rule_key', ''),
                        finding_json=json.dumps(single_json, ensure_ascii=False),
                        task_name=getattr(task, 'name', ''),
                        task_content=getattr(task, 'content', ''),
                        task_business_flow_code=getattr(task, 'business_flow_code', ''),
                        task_contract_code=getattr(task, 'contract_code', ''),
                        task_start_line=getattr(task, 'start_line', ''),
                        task_end_line=getattr(task, 'end_line', ''),
                        task_relative_file_path=getattr(task, 'relative_file_path', ''),
                        task_absolute_file_path=getattr(task, 'absolute_file_path', ''),
                        task_rule=getattr(task, 'rule', ''),
                        task_group=getattr(task, 'group', ''),
                        dedup_status='kept',
                        validation_status='pending',
                        validation_record='',
                    )
                )

            if findings:
                finding_mgr.add_findings(findings, commit=True)

            # æ ‡è®°æ‹†åˆ†å®Œæˆ
            task_manager.update_short_result(task.id, "split_done")
        except Exception as e:
            self.logger.warning(f"æ‹†åˆ†å†™å…¥ finding å¤±è´¥: {e}")
            try:
                if getattr(task, 'id', None):
                    task_manager.update_short_result(task.id, "split_failed")
            except Exception:
                pass
    
    def _get_group_results_summary(self, task, task_manager) -> str:
        """è·å–åŒç»„ä»»åŠ¡çš„ç»“æœæ€»ç»“"""
        try:
            # è·å–ä»»åŠ¡çš„group UUID
            group_uuid = getattr(task, 'group', None)
            if not group_uuid or group_uuid.strip() == "":
                return ""
            
            # æŸ¥è¯¢åŒç»„ä¸­å·²æœ‰ç»“æœçš„ä»»åŠ¡
            tasks_with_results = task_manager.query_tasks_with_results_by_group(group_uuid)
            if not tasks_with_results:
                return ""
            
            # æ’é™¤å½“å‰ä»»åŠ¡è‡ªå·±
            current_task_uuid = getattr(task, 'uuid', None)
            if current_task_uuid:
                tasks_with_results = [t for t in tasks_with_results if t.uuid != current_task_uuid]
            
            if not tasks_with_results:
                return ""
            
            # ä½¿ç”¨ç»“æœæ€»ç»“å™¨ç”Ÿæˆæ€»ç»“
            from .utils.group_result_summarizer import GroupResultSummarizer
            summary = GroupResultSummarizer.summarize_group_results(tasks_with_results)
            
            if summary:
                print(f"ğŸ” ä¸ºä»»åŠ¡ {task.name} æ‰¾åˆ° {len(tasks_with_results)} ä¸ªåŒç»„å·²å®Œæˆä»»åŠ¡çš„ç»“æœæ€»ç»“")
            
            return summary or ""
        except Exception as e:
            self.logger.warning(f"è·å–åŒç»„ç»“æœæ€»ç»“å¤±è´¥: {e}")
            return ""

    def _assemble_prompt_with_specific_rule(self, code: str, rule_list: list, rule_key: str) -> str:
        """ä½¿ç”¨å…·ä½“çš„ruleåˆ—è¡¨ç»„è£…prompt"""
        
        # ğŸ¯ ä¸“é—¨å¤„ç†assumption_violationç±»å‹çš„ä»»åŠ¡
        if rule_key == "assumption_violation":
            # å¯¹äºassumptionéªŒè¯ï¼Œrule_listæ˜¯åˆ—è¡¨æ ¼å¼ï¼ˆä¸€ç»„assumption/invariantï¼Œæœ€å¤š3ä¸ªï¼‰
            # ç›´æ¥ä½¿ç”¨ä¸“é—¨çš„assumptionéªŒè¯prompt
            return AssumptionValidationPrompt.get_assumption_validation_prompt(
                code, rule_list
            )
        
        # ğŸ¯ ä¸“é—¨å¤„ç†PURE_SCANç±»å‹çš„ä»»åŠ¡
        if rule_key == "PURE_SCAN":
            # ä½¿ç”¨pure scançš„promptç»„è£…å™¨
            return PromptAssembler.assemble_prompt_pure(code)
        
        # åŸæœ‰çš„æ¼æ´æ‰«æé€»è¾‘ï¼ˆéassumptionç±»å‹ï¼‰
        else:
            # å…³é”®ï¼šä¸è¦å†æ‹¼æ¥ PeripheryPrompt.guidelines/jailbreak_prompt
            # è¿™äº›é€šç”¨å°¾å·´ä¼šå¼•å…¥é JSON çš„è¾“å‡ºæ ¼å¼è¦æ±‚ï¼Œå¯¼è‡´æ¨¡å‹å€¾å‘åªè¾“å‡º 1 æ¡ç”šè‡³è¾“å‡ºå¼‚å¸¸ã€‚
            return (
                VulReasoningJsonPrompt.build_prompt(
                code=code,
                rule_key=rule_key,
                rule_list=rule_list,
                group_summary="",  # å½“å‰é»˜è®¤ä¸ä½¿ç”¨å†å²
                )
                + "\n"
                + PeripheryPrompt.guidelines_json_only()
            )